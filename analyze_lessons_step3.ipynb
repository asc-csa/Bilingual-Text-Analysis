{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing CSA's Lessons Learned | Analyse des leçons apprises par l'ASC \r\n",
    "### Step 3: Topic Modelling | Étape 3 : Modélisation des sujets  \r\n",
    "This notebook takes the results of step 2 (translated text, sentiment analysis scores) and applies latent Dirichlet allocation (LDA) for topic modelling. \r\n",
    "This workflow could be adapted to any other spreadsheet or csv. All that is needed as input is a spreadsheet with a column of text in English, French, or both.  \r\n",
    "\r\n",
    "Ce cahier reprend les résultats de l'étape 2 (texte traduit, analyse de sentiments) et y applique l'allocation de Dirichlet latente (LDA) pour faire la modélisation des sujets. \r\n",
    "Ce workflow pourrait être adapté à tout autre tableur ou csv. Tout ce qui est requis c'est un tableur qui contient une colonne de texte en anglais, français, ou les deux langues. \r\n",
    "\r\n",
    "Author/Auteur: N Fee, Canadian Space Agency/Agence spatiale canadienne, 2021-06-18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\DevSoftware\\Anaconda38\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk\r\n",
    "import pandas as pd \r\n",
    "\r\n",
    "\r\n",
    "#Libraries for topic modelling \r\n",
    "import gensim.corpora as corpora\r\n",
    "from gensim.utils import simple_preprocess\r\n",
    "from gensim.models import CoherenceModel\r\n",
    "import gensim \r\n",
    "\r\n",
    "\r\n",
    "import spacy #for lemmatization (ie. grouping together inflected forms of a word - studying and studious become study, etc...)  \r\n",
    "import pyLDAvis.gensim_models # for visualizing the topics\r\n",
    "import pyLDAvis\r\n",
    "\r\n",
    "import operator #useful for dealing with topic tuples\r\n",
    "import warnings #useful for disabling an annoying deprecation warning \r\n",
    "\r\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning) #otherwise one of the libraries keeps logging errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = \"2_Output/LessonsLearned_step2.xlsx\"\r\n",
    "outfile = \"2_Output/LessonsLearned_step3.xlsx\"\r\n",
    "\r\n",
    "stopwords_en = nltk.corpus.stopwords.words(\"english\") #stopwords are words that should be removed before analysis (generally because they are common - 'the', 'a', etc...)\r\n",
    "stopwords_fr = nltk.corpus.stopwords.words(\"french\") #same but in french\r\n",
    "stopwords_bil = stopwords_en + stopwords_fr #same but including stopwords from english and french. This is for analysing the untranslated text\r\n",
    "\r\n",
    "newStopWords_en = ['csa','project','projects','also', 'agency', 'space'] #New stopwords relevant for the CSA dataset\r\n",
    "newStopWords_fr = ['asc','projet','projets', 'cette','agence', 'spatial']\r\n",
    "\r\n",
    "#Add the new stopwords to the existing stopwords\r\n",
    "stopwords_en.extend(newStopWords_en)\r\n",
    "stopwords_fr.extend(newStopWords_fr)\r\n",
    "stopwords_bil.extend(newStopWords_en+newStopWords_fr)\r\n",
    "                     \r\n",
    "#More user-friendly column names for the results. If you change the number of topics, you'll want to change this as well.                      \r\n",
    "colnames = ['Topic 1', 'Topic 1 Probability',\r\n",
    "            'Topic 2', 'Topic 2 Probability',\r\n",
    "            'Topic 3', 'Topic 3 Probability',\r\n",
    "            'Topic 4', 'Topic 4 Probability',\r\n",
    "            'Topic 5', 'Topic 5 Probability',\r\n",
    "            'Topic 6', 'Topic 6 Probability',\r\n",
    "            'Top Topic', 'Top Topic Probability']\r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizes the text (ie. each word is an element in a list - this is necessary for analysis)\r\n",
    "def sentence_to_words(sentences):\r\n",
    "    for sentence in sentences:\r\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True to remove punctuation\r\n",
    "        \r\n",
    "# Remove the stopwords from the tokenized text\r\n",
    "def remove_stopwords(row,colname,stop_words):\r\n",
    "    text = str(row[colname])\r\n",
    "    text_clean = simple_preprocess(text)\r\n",
    "    return [word for word in text_clean if word not in stop_words]\r\n",
    "\r\n",
    "#Lemmatize the tokenized text (ie. grouping together inflected forms of a word - studying and studious become study, etc...)\r\n",
    "def lemmatization(texts, nlp_lang, allowed_postags=['NOUN','VERB', 'ADJ','ADV']):\r\n",
    "    text_ls = []\r\n",
    "    for sent in texts:\r\n",
    "        doc = nlp_lang(\" \".join(sent)) \r\n",
    "        text_ls.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\r\n",
    "    return text_ls\r\n",
    "\r\n",
    "#Create a corpus and dictionary based on the lemmatized text (very simply put, prep the text for the lda model). More info about this here: https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#core-concepts-corpus (external site in english only)   \r\n",
    "def lemmatized_to_corpus(data_lemmatized):\r\n",
    "    id2word = corpora.Dictionary(data_lemmatized)\r\n",
    "    texts = data_lemmatized\r\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\r\n",
    "    return corpus, id2word\r\n",
    "\r\n",
    "#Build the latent dirichlet allocation (LDA) model. This is being used for topic modelling. \r\n",
    "def build_lda(corpus,id2word,ntopics=10):\r\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus= corpus,\r\n",
    "                                                   id2word=id2word,\r\n",
    "                                                   num_topics=ntopics, \r\n",
    "                                                   random_state=100,\r\n",
    "                                                   update_every=1,\r\n",
    "                                                   chunksize=100,\r\n",
    "                                                   passes=100,\r\n",
    "                                                   alpha='auto',\r\n",
    "                                                   per_word_topics=True)\r\n",
    "    return lda_model\r\n",
    "\r\n",
    "#Compute the coherence score. \r\n",
    "def compute_coherence(ldamodel,data_lemmatized,id2word):\r\n",
    "    coherence_model_lda = CoherenceModel(model=ldamodel, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\r\n",
    "    coherence_lda = coherence_model_lda.get_coherence()\r\n",
    "    return coherence_lda\r\n",
    "\r\n",
    "#Compute the perplexity score and print it and the coherence score \r\n",
    "def print_perplexity_coherence(ldamodel,corpus, title, data_lemmatized, id2word):\r\n",
    "    coherence_lda = compute_coherence(ldamodel,data_lemmatized, id2word)\r\n",
    "    print(title)\r\n",
    "    print('Perplexity:      %6.3f'%ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\r\n",
    "    print('Coherence Score: %6.3f \\n'%coherence_lda)\r\n",
    "\r\n",
    "# Based on a list of topics, determine the topic that the text probably falls under\r\n",
    "def assign_topic(lda_model,row,corpus_col):\r\n",
    "    topic_list = lda_model.get_document_topics(row[corpus_col], minimum_probability=0, minimum_phi_value=None, per_word_topics=False)    \r\n",
    "    top_topic = max(topic_list, key=operator.itemgetter(1))\r\n",
    "    topic_list.append(top_topic)\r\n",
    "    \r\n",
    "    topic_list = [(item[0],round(item[1],2)) for item in topic_list] #round the probabilities \r\n",
    "    topic_list  = [item for t in topic_list for item in t] #list of tuples to a flat list \r\n",
    "    return topic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [honoured, present, state, canadian, sector, r...\n",
      "1     [efforts, soutien, innovation, collaboration, ...\n",
      "2     [several, differing, programs, services, csin,...\n",
      "3     [encourager, lancement, nouvelles, entreprises...\n",
      "4     [research, development, expenditures, totalled...\n",
      "5     [plan, proposes, commitment, fund, initial, th...\n",
      "6     [réseau, innovation, canadien, risc, fournira,...\n",
      "7     [better, reflect, current, best, practices, ma...\n",
      "8     [plan, opérationnel, décrit, détails, personne...\n",
      "9     [gouvernement, canada, appuie, depuis, longtem...\n",
      "10    [order, measure, changes, taking, place, canad...\n",
      "11    [parallèlement, efforts, concertés, ocde, mins...\n",
      "12    [nombre, travailleurs, élevé, dont, personnes,...\n",
      "13    [importe, noter, toutes, informations, relativ...\n",
      "14    [past, five, years, total, revenues, quebec, n...\n",
      "15    [following, table, reports, percentage, highly...\n",
      "16    [universités, centres, recherche, continué, fa...\n",
      "17    [main, œuvre, secteur, canadien, connu, expans...\n",
      "18    [plus, importants, marchés, exportation, organ...\n",
      "Name: lesson_clean_bil, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Remove stopwords\n",
    "df['lesson_clean_en'] = df.apply(lambda row: list(remove_stopwords(row,'Lessons EN',stopwords_en)), axis=1)\n",
    "df['lesson_clean_fr'] = df.apply(lambda row: list(remove_stopwords(row,'Lessons FR',stopwords_fr)), axis=1)\n",
    "df['lesson_clean_bil'] = df.apply(lambda row: list(remove_stopwords(row,'Lesson Learned',stopwords_bil)), axis=1)\n",
    "print(df['lesson_clean_bil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [honour, preser, state, canadian, report, cove...\n",
      "1     [soutien, innovation, collaboration, manifeste...\n",
      "2     [several, differ, program, service, csin, offe...\n",
      "3     [encourager, lancement, nouveau, entreprise, s...\n",
      "4     [development, expenditur, total, organization,...\n",
      "5     [plan, propose, commitment, fund, initial, yea...\n",
      "6     [réseau, innovation, canadien, risc, fournir, ...\n",
      "7     [well, reflect, current, good, practic, matter...\n",
      "8     [plan, opérationnel, décrire, détail, personne...\n",
      "9     [gouvernement, canader, appuie, longtemp, init...\n",
      "10    [order, measur, change, place, canader, sector...\n",
      "11    [parallèlement, effort, concerter, ocd, minstè...\n",
      "12    [nombre, travailleur, élevé, personn, hautemen...\n",
      "13    [importe, noter, information, relatif, organis...\n",
      "14    [year, total, revenir, quebec, nearly, double,...\n",
      "15    [follow, table, report, percentage, highly, qu...\n",
      "16    [université, centre, recherche, continuer, sou...\n",
      "17    [main, œuvre, secteur, canadien, connaître, ex...\n",
      "18    [important, marché, exportation, organisme, vo...\n",
      "Name: lemmatized_bil, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Initialize spacy 'en' and 'fr' models. You only need to do this once. \r\n",
    "#python -m spacy download fr_core_news_sm\r\n",
    "#python -m spacy download en_core_web_sm\r\n",
    "\r\n",
    "\r\n",
    "nlp_en = spacy.load(\"en_core_web_sm\")\r\n",
    "nlp_fr = spacy.load(\"fr_core_news_sm\") \r\n",
    "\r\n",
    "\r\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\r\n",
    "df['lemmatized_en'] = lemmatization(df['lesson_clean_en'], nlp_en)\r\n",
    "df['lemmatized_fr'] = lemmatization(df['lesson_clean_fr'], nlp_fr)\r\n",
    "\r\n",
    "# Some fancy footwork to lemmatize the bilingual text\r\n",
    "df['lemmatized_bil'] = lemmatization(df['lesson_clean_bil'], nlp_en)\r\n",
    "df['lemmatized_bil'] = lemmatization(df['lemmatized_bil'], nlp_fr)\r\n",
    "\r\n",
    "print(df['lemmatized_bil'])\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     [(0, 2), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1...\n",
      "1     [(3, 1), (30, 1), (31, 1), (32, 1), (33, 1), (...\n",
      "2     [(11, 1), (77, 1), (78, 1), (79, 1), (80, 1), ...\n",
      "3     [(49, 1), (56, 1), (57, 1), (67, 2), (71, 1), ...\n",
      "4     [(0, 1), (20, 3), (26, 1), (91, 1), (122, 1), ...\n",
      "5     [(39, 1), (59, 1), (79, 1), (83, 1), (90, 1), ...\n",
      "6     [(16, 1), (37, 1), (50, 1), (64, 1), (66, 2), ...\n",
      "7     [(0, 1), (4, 1), (24, 1), (126, 1), (142, 1), ...\n",
      "8     [(49, 1), (55, 1), (57, 1), (59, 2), (64, 2), ...\n",
      "9     [(38, 1), (47, 1), (50, 1), (67, 2), (71, 1), ...\n",
      "10    [(4, 1), (20, 1), (24, 2), (91, 2), (143, 3), ...\n",
      "11    [(42, 1), (50, 1), (59, 1), (67, 2), (72, 1), ...\n",
      "12    [(37, 3), (58, 1), (67, 3), (72, 2), (93, 1), ...\n",
      "13    [(16, 1), (255, 1), (275, 1), (290, 1), (299, ...\n",
      "14    [(129, 1), (140, 1), (143, 4), (153, 1), (330,...\n",
      "15    [(4, 1), (24, 1), (118, 1), (140, 1), (182, 1)...\n",
      "16    [(5, 2), (37, 1), (45, 2), (47, 1), (72, 2), (...\n",
      "17    [(37, 1), (56, 1), (67, 1), (140, 1), (162, 1)...\n",
      "18    [(30, 1), (33, 1), (72, 1), (96, 1), (240, 1),...\n",
      "Name: corpus_bil, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Create corpora based on the lemmatized text (ie. a vector where each word is described by an ID and by the frequency it appears in the text)\r\n",
    "df['corpus_en'],id2word_en = lemmatized_to_corpus(df['lemmatized_en'])\r\n",
    "df['corpus_fr'],id2word_fr = lemmatized_to_corpus(df['lemmatized_fr'])\r\n",
    "df['corpus_bil'],id2word_bil = lemmatized_to_corpus(df['lemmatized_bil'])\r\n",
    "print(df['corpus_bil'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH\n",
      "Perplexity:      -6.145\n",
      "Coherence Score:  0.426 \n",
      "\n",
      "FRENCH\n",
      "Perplexity:      -6.233\n",
      "Coherence Score:  0.455 \n",
      "\n",
      "BILINGUAL\n",
      "Perplexity:      -6.314\n",
      "Coherence Score:  0.487 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build the topic models\r\n",
    "lda_model_en = build_lda(df['corpus_en'], id2word_en, ntopics = 6)\r\n",
    "lda_model_fr = build_lda(df['corpus_fr'], id2word_fr, ntopics = 6)\r\n",
    "lda_model_bil = build_lda(df['corpus_bil'], id2word_bil, ntopics = 6)\r\n",
    "\r\n",
    "# Determine how successful the topic models are (ideally we want low perplexity, high coherence)\r\n",
    "print_perplexity_coherence(lda_model_en,df['corpus_en'],'ENGLISH', df['lemmatized_en'], id2word_en)\r\n",
    "print_perplexity_coherence(lda_model_fr,df['corpus_fr'],'FRENCH', df['lemmatized_fr'], id2word_fr)\r\n",
    "print_perplexity_coherence(lda_model_bil,df['corpus_bil'],'BILINGUAL', df['lemmatized_bil'], id2word_bil)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH Topics\n",
      "[(0, '0.029*\"year\" + 0.025*\"increase\" + 0.020*\"new\" + 0.019*\"canadian\" + 0.019*\"innovation\" + 0.019*\"support\" + 0.015*\"growth\" + 0.015*\"provide\" + 0.015*\"revenue\" + 0.015*\"network\"'), (1, '0.039*\"export\" + 0.027*\"europe\" + 0.027*\"state\" + 0.014*\"market\" + 0.014*\"increase\" + 0.014*\"large\" + 0.014*\"continue\" + 0.014*\"organization\" + 0.014*\"year\" + 0.014*\"row\"'), (2, '0.028*\"sector\" + 0.026*\"organization\" + 0.017*\"year\" + 0.016*\"expenditure\" + 0.016*\"research\" + 0.016*\"activity\" + 0.016*\"canada\" + 0.012*\"support\" + 0.012*\"canadian\" + 0.011*\"result\"'), (3, '0.020*\"service\" + 0.015*\"program\" + 0.015*\"study\" + 0.015*\"activity\" + 0.015*\"hqp\" + 0.015*\"sector\" + 0.015*\"member\" + 0.015*\"canadian\" + 0.015*\"research\" + 0.010*\"center\"'), (4, '0.033*\"sector\" + 0.025*\"year\" + 0.017*\"report\" + 0.017*\"business\" + 0.017*\"access\" + 0.017*\"program\" + 0.017*\"facilitate\" + 0.009*\"generally\" + 0.009*\"order\" + 0.009*\"measure\"'), (5, '0.018*\"sector\" + 0.018*\"canadian\" + 0.012*\"activity\" + 0.012*\"node\" + 0.012*\"plan\" + 0.012*\"operation\" + 0.012*\"downstream\" + 0.012*\"kernel\" + 0.012*\"datum\" + 0.012*\"network\"')]\n",
      "\n",
      "FRENCH Topics\n",
      "[(0, '0.016*\"méthodologie\" + 0.016*\"soutien\" + 0.016*\"secteur\" + 0.016*\"réseau\" + 0.016*\"fournir\" + 0.016*\"nouveau\" + 0.016*\"canadien\" + 0.009*\"cohérence\" + 0.009*\"faire\" + 0.009*\"donnée\"'), (1, '0.042*\"année\" + 0.028*\"spatial\" + 0.019*\"dernier\" + 0.015*\"rapport\" + 0.015*\"innovation\" + 0.015*\"secteur\" + 0.015*\"exportation\" + 0.015*\"canadien\" + 0.015*\"passer\" + 0.010*\"canada\"'), (2, '0.022*\"spatial\" + 0.022*\"canadien\" + 0.022*\"secteur\" + 0.015*\"information\" + 0.015*\"organisation\" + 0.015*\"activité\" + 0.015*\"rapport\" + 0.015*\"employer\" + 0.008*\"domaine\" + 0.008*\"demande\"'), (3, '0.022*\"service\" + 0.022*\"programme\" + 0.022*\"réseau\" + 0.022*\"objectif\" + 0.017*\"membre\" + 0.017*\"existant\" + 0.017*\"plan\" + 0.017*\"pouvoir\" + 0.012*\"csin\" + 0.012*\"tirer\"'), (4, '0.032*\"secteur\" + 0.019*\"spatial\" + 0.019*\"activité\" + 0.016*\"recherche\" + 0.016*\"canadien\" + 0.010*\"étranger\" + 0.010*\"étude\" + 0.010*\"aval\" + 0.010*\"dépense\" + 0.010*\"obtenir\"'), (5, '0.023*\"rapport\" + 0.016*\"pouvoir\" + 0.016*\"plus\" + 0.016*\"présent\" + 0.016*\"employer\" + 0.016*\"utiliser\" + 0.016*\"phq\" + 0.016*\"fin\" + 0.009*\"ingénieur\" + 0.009*\"inclure\"')]\n",
      "\n",
      "BILINGUAL Topics\n",
      "[(0, '0.020*\"recherche\" + 0.020*\"source\" + 0.014*\"université\" + 0.014*\"européen\" + 0.014*\"réseau\" + 0.014*\"activité\" + 0.014*\"fondre\" + 0.014*\"spatial\" + 0.014*\"étranger\" + 0.014*\"soutien\"'), (1, '0.019*\"objectif\" + 0.013*\"note\" + 0.013*\"nœud\" + 0.013*\"fonctionnement\" + 0.013*\"position\" + 0.013*\"hqp\" + 0.013*\"réseau\" + 0.013*\"noyau\" + 0.013*\"plan\" + 0.013*\"personnel\"'), (2, '0.022*\"donné\" + 0.022*\"secteur\" + 0.022*\"canadien\" + 0.015*\"aval\" + 0.015*\"spatial\" + 0.015*\"référence\" + 0.015*\"activité\" + 0.008*\"habituellement\" + 0.008*\"hautement\" + 0.008*\"important\"'), (3, '0.027*\"secteur\" + 0.022*\"innovation\" + 0.022*\"spatial\" + 0.016*\"économique\" + 0.011*\"soutien\" + 0.011*\"faciliter\" + 0.011*\"effort\" + 0.011*\"infrastructur\" + 0.011*\"nouveau\" + 0.011*\"plan\"'), (4, '0.040*\"year\" + 0.023*\"report\" + 0.018*\"canadian\" + 0.018*\"service\" + 0.018*\"program\" + 0.014*\"member\" + 0.014*\"revenir\" + 0.014*\"organization\" + 0.014*\"activity\" + 0.014*\"sector\"'), (5, '0.015*\"total\" + 0.015*\"secteur\" + 0.015*\"organization\" + 0.015*\"expenditur\" + 0.010*\"canader\" + 0.010*\"segment\" + 0.010*\"étude\" + 0.010*\"lorsqu\" + 0.010*\"invention\" + 0.010*\"organisme\"')]\n"
     ]
    }
   ],
   "source": [
    "print('ENGLISH Topics')\r\n",
    "print(lda_model_en.print_topics())\r\n",
    "\r\n",
    "print('\\nFRENCH Topics')\r\n",
    "print(lda_model_fr.print_topics())\r\n",
    "\r\n",
    "print('\\nBILINGUAL Topics')\r\n",
    "print(lda_model_bil.print_topics())\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\r\n",
    "pyLDAvis.enable_notebook()\r\n",
    "vis_en = pyLDAvis.gensim_models.prepare(lda_model_en, df['corpus_en'], id2word_en, sort_topics = False)\r\n",
    "pyLDAvis.save_html(vis_en, '2_Output/LessonsLearned_topics_EN.html')\r\n",
    "\r\n",
    "vis_fr = pyLDAvis.gensim_models.prepare(lda_model_fr, df['corpus_fr'], id2word_fr,sort_topics = False)\r\n",
    "pyLDAvis.save_html(vis_fr, '2_Output/LessonsLearned_topics_FR.html')\r\n",
    "\r\n",
    "vis_bil = pyLDAvis.gensim_models.prepare(lda_model_bil, df['corpus_bil'], id2word_bil,sort_topics = False)\r\n",
    "pyLDAvis.save_html(vis_bil, '2_Output/LessonsLearned_topics_BIL.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Topic 1  Topic 1 Probability  Topic 2  Topic 2 Probability  Topic 3  \\\n",
      "0         0                 0.00        1                  0.0        2   \n",
      "1         0                 0.81        1                  0.0        2   \n",
      "2         0                 0.00        1                  0.0        2   \n",
      "3         0                 0.00        1                  0.0        2   \n",
      "4         0                 0.00        1                  0.0        2   \n",
      "5         0                 0.00        1                  0.0        2   \n",
      "6         0                 1.00        1                  0.0        2   \n",
      "7         0                 1.00        1                  0.0        2   \n",
      "8         0                 0.00        1                  0.0        2   \n",
      "9         0                 0.00        1                  0.0        2   \n",
      "10        0                 0.00        1                  0.0        2   \n",
      "11        0                 0.00        1                  0.0        2   \n",
      "12        0                 0.00        1                  0.0        2   \n",
      "13        0                 0.00        1                  0.0        2   \n",
      "14        0                 1.00        1                  0.0        2   \n",
      "15        0                 0.00        1                  0.0        2   \n",
      "16        0                 0.00        1                  0.0        2   \n",
      "17        0                 1.00        1                  0.0        2   \n",
      "18        0                 0.00        1                  1.0        2   \n",
      "\n",
      "    Topic 3 Probability  Topic 4  Topic 4 Probability  Topic 5  \\\n",
      "0                  1.00        3                  0.0        4   \n",
      "1                  0.19        3                  0.0        4   \n",
      "2                  0.00        3                  1.0        4   \n",
      "3                  0.00        3                  0.0        4   \n",
      "4                  1.00        3                  0.0        4   \n",
      "5                  1.00        3                  0.0        4   \n",
      "6                  0.00        3                  0.0        4   \n",
      "7                  0.00        3                  0.0        4   \n",
      "8                  0.00        3                  0.0        4   \n",
      "9                  1.00        3                  0.0        4   \n",
      "10                 0.00        3                  0.0        4   \n",
      "11                 0.00        3                  1.0        4   \n",
      "12                 0.00        3                  0.0        4   \n",
      "13                 0.00        3                  1.0        4   \n",
      "14                 0.00        3                  0.0        4   \n",
      "15                 0.00        3                  1.0        4   \n",
      "16                 0.00        3                  1.0        4   \n",
      "17                 0.00        3                  0.0        4   \n",
      "18                 0.00        3                  0.0        4   \n",
      "\n",
      "    Topic 5 Probability  Topic 6  Topic 6 Probability  Top Topic  \\\n",
      "0                   0.0        5                  0.0          2   \n",
      "1                   0.0        5                  0.0          0   \n",
      "2                   0.0        5                  0.0          3   \n",
      "3                   1.0        5                  0.0          4   \n",
      "4                   0.0        5                  0.0          2   \n",
      "5                   0.0        5                  0.0          2   \n",
      "6                   0.0        5                  0.0          0   \n",
      "7                   0.0        5                  0.0          0   \n",
      "8                   0.0        5                  1.0          5   \n",
      "9                   0.0        5                  0.0          2   \n",
      "10                  1.0        5                  0.0          4   \n",
      "11                  0.0        5                  0.0          3   \n",
      "12                  0.0        5                  1.0          5   \n",
      "13                  0.0        5                  0.0          3   \n",
      "14                  0.0        5                  0.0          0   \n",
      "15                  0.0        5                  0.0          3   \n",
      "16                  0.0        5                  0.0          3   \n",
      "17                  0.0        5                  0.0          0   \n",
      "18                  0.0        5                  0.0          1   \n",
      "\n",
      "    Top Topic Probability  \n",
      "0                    1.00  \n",
      "1                    0.81  \n",
      "2                    1.00  \n",
      "3                    1.00  \n",
      "4                    1.00  \n",
      "5                    1.00  \n",
      "6                    1.00  \n",
      "7                    1.00  \n",
      "8                    1.00  \n",
      "9                    1.00  \n",
      "10                   1.00  \n",
      "11                   1.00  \n",
      "12                   1.00  \n",
      "13                   1.00  \n",
      "14                   1.00  \n",
      "15                   1.00  \n",
      "16                   1.00  \n",
      "17                   1.00  \n",
      "18                   1.00  \n"
     ]
    }
   ],
   "source": [
    "# Assign topics to each text based on the trained model\r\n",
    "topics = pd.DataFrame(df.apply(lambda row: assign_topic(lda_model_en,row,'corpus_en'), axis=1), columns = ['topics'])\r\n",
    "\r\n",
    "# Useful for formatting \r\n",
    "topics_df=  pd.DataFrame(topics['topics'].to_list(), columns=colnames)\r\n",
    "\r\n",
    "print(topics_df)\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0                                     Lesson Learned Language  \\\n",
      "0            0  I am honoured to present the State of the Cana...       en   \n",
      "1            1  Les efforts de soutien à l’innovation et de co...       fr   \n",
      "2            2  There are several differing programs and servi...       en   \n",
      "3            3  Encourager le lancement de nouvelles entrepris...       fr   \n",
      "4            4  Research and development (R&D) expenditures to...       en   \n",
      "5            5  The plan proposes a commitment to fund the ini...       en   \n",
      "6            6  Le Réseau d’innovation spatial canadien (RISC)...       fr   \n",
      "7            7  To better reflect the current best practices a...       en   \n",
      "8            8  Le plan opérationnel décrit les détails sur le...       fr   \n",
      "9            9  Le gouvernement du Canada appuie depuis longte...       fr   \n",
      "10          10  In order to measure the changes taking place i...       en   \n",
      "11          11  Parallèlement aux efforts concertés avec l’OCD...       fr   \n",
      "12          12  Le nombre de travailleurs s’est élevé à 8 200,...       fr   \n",
      "13          13  Il importe de noter que toutes les information...       fr   \n",
      "14          14  Over the past five years, total revenues in Qu...       en   \n",
      "15          15  The following table reports the percentage of ...       en   \n",
      "16          16  En 2013, les universités et les centres de rec...       fr   \n",
      "17          17  La main-d’œuvre du secteur spatial canadien a ...       fr   \n",
      "18          18  Les plus importants marchés d’exportation des ...       fr   \n",
      "\n",
      "                                           Lessons EN  \\\n",
      "0   I am honoured to present the State of the Cana...   \n",
      "1   Efforts to support innovation and collaboratio...   \n",
      "2   There are several differing programs and servi...   \n",
      "3   Encourage the launch of new businesses in the ...   \n",
      "4   Research and development (R&D) expenditures to...   \n",
      "5   The plan proposes a commitment to fund the ini...   \n",
      "6   The Canadian Space Innovation Network (RISC) w...   \n",
      "7   To better reflect the current best practices a...   \n",
      "8   The operational plan describes the details of ...   \n",
      "9   The Government of Canada has a long history of...   \n",
      "10  In order to measure the changes taking place i...   \n",
      "11  In addition to concerted efforts with the OECD...   \n",
      "12  The number of workers rose to 8,200, of which ...   \n",
      "13  It is important to note that all information r...   \n",
      "14  Over the past five years, total revenues in Qu...   \n",
      "15  The following table reports the percentage of ...   \n",
      "16  In 2013, universities and research centers con...   \n",
      "17  The Canadian space sector workforce expanded i...   \n",
      "18  The largest export markets for space-based org...   \n",
      "\n",
      "                                           Lessons FR  Negative Sentiment  \\\n",
      "0   J'ai l'honneur de présenter le rapport sur l'é...               0.000   \n",
      "1   Les efforts de soutien à l’innovation et de co...               0.013   \n",
      "2   Il existe plusieurs programmes et services dif...               0.000   \n",
      "3   Encourager le lancement de nouvelles entrepris...               0.000   \n",
      "4   Les dépenses de recherche et développement (R&...               0.000   \n",
      "5   Le plan propose un engagement à financer les t...               0.000   \n",
      "6   Le Réseau d’innovation spatial canadien (RISC)...               0.032   \n",
      "7   Afin de mieux refléter les meilleures pratique...               0.000   \n",
      "8   Le plan opérationnel décrit les détails sur le...               0.000   \n",
      "9   Le gouvernement du Canada appuie depuis longte...               0.040   \n",
      "10  Afin de mesurer les changements qui s'opèrent ...               0.000   \n",
      "11  Parallèlement aux efforts concertés avec l’OCD...               0.000   \n",
      "12  Le nombre de travailleurs s’est élevé à 8 200,...               0.000   \n",
      "13  Il importe de noter que toutes les information...               0.000   \n",
      "14  Au cours des cinq dernières années, les revenu...               0.000   \n",
      "15  Le tableau suivant présente le pourcentage de ...               0.000   \n",
      "16  En 2013, les universités et les centres de rec...               0.000   \n",
      "17  La main-d’œuvre du secteur spatial canadien a ...               0.031   \n",
      "18  Les plus importants marchés d’exportation des ...               0.000   \n",
      "\n",
      "    Neutral Sentiment  Positive Sentiment  Compound Sentiment Score  \\\n",
      "0               0.932               0.068                    0.6369   \n",
      "1               0.822               0.164                    0.9184   \n",
      "2               0.950               0.050                    0.5095   \n",
      "3               0.860               0.140                    0.8225   \n",
      "4               0.922               0.078                    0.6705   \n",
      "5               0.858               0.142                    0.7003   \n",
      "6               0.698               0.270                    0.9072   \n",
      "7               0.820               0.180                    0.8934   \n",
      "8               0.977               0.023                    0.3818   \n",
      "9               0.734               0.226                    0.8934   \n",
      "10              1.000               0.000                    0.0000   \n",
      "11              0.875               0.125                    0.8126   \n",
      "12              0.885               0.115                    0.9245   \n",
      "13              0.923               0.077                    0.5994   \n",
      "14              0.914               0.086                    0.7003   \n",
      "15              0.977               0.023                    0.0772   \n",
      "16              0.966               0.034                    0.2263   \n",
      "17              0.758               0.211                    0.7783   \n",
      "18              0.815               0.185                    0.7717   \n",
      "\n",
      "                                      lesson_clean_en  ... Topic 3  \\\n",
      "0   [honoured, present, state, canadian, sector, r...  ...       2   \n",
      "1   [efforts, support, innovation, collaboration, ...  ...       2   \n",
      "2   [several, differing, programs, services, csin,...  ...       2   \n",
      "3   [encourage, launch, new, businesses, sector, d...  ...       2   \n",
      "4   [research, development, expenditures, totalled...  ...       2   \n",
      "5   [plan, proposes, commitment, fund, initial, th...  ...       2   \n",
      "6   [canadian, innovation, network, risc, provide,...  ...       2   \n",
      "7   [better, reflect, current, best, practices, ma...  ...       2   \n",
      "8   [operational, plan, describes, details, person...  ...       2   \n",
      "9   [government, canada, long, history, supporting...  ...       2   \n",
      "10  [order, measure, changes, taking, place, canad...  ...       2   \n",
      "11  [addition, concerted, efforts, oecd, ministry,...  ...       2   \n",
      "12  [number, workers, rose, highly, skilled, peopl...  ...       2   \n",
      "13  [important, note, information, relating, organ...  ...       2   \n",
      "14  [past, five, years, total, revenues, quebec, n...  ...       2   \n",
      "15  [following, table, reports, percentage, highly...  ...       2   \n",
      "16  [universities, research, centers, continued, r...  ...       2   \n",
      "17  [canadian, sector, workforce, expanded, new, p...  ...       2   \n",
      "18  [largest, export, markets, based, organization...  ...       2   \n",
      "\n",
      "   Topic 3 Probability Topic 4 Topic 4 Probability Topic 5  \\\n",
      "0                 1.00       3                 0.0       4   \n",
      "1                 0.19       3                 0.0       4   \n",
      "2                 0.00       3                 1.0       4   \n",
      "3                 0.00       3                 0.0       4   \n",
      "4                 1.00       3                 0.0       4   \n",
      "5                 1.00       3                 0.0       4   \n",
      "6                 0.00       3                 0.0       4   \n",
      "7                 0.00       3                 0.0       4   \n",
      "8                 0.00       3                 0.0       4   \n",
      "9                 1.00       3                 0.0       4   \n",
      "10                0.00       3                 0.0       4   \n",
      "11                0.00       3                 1.0       4   \n",
      "12                0.00       3                 0.0       4   \n",
      "13                0.00       3                 1.0       4   \n",
      "14                0.00       3                 0.0       4   \n",
      "15                0.00       3                 1.0       4   \n",
      "16                0.00       3                 1.0       4   \n",
      "17                0.00       3                 0.0       4   \n",
      "18                0.00       3                 0.0       4   \n",
      "\n",
      "   Topic 5 Probability Topic 6 Topic 6 Probability  Top Topic  \\\n",
      "0                  0.0       5                 0.0          2   \n",
      "1                  0.0       5                 0.0          0   \n",
      "2                  0.0       5                 0.0          3   \n",
      "3                  1.0       5                 0.0          4   \n",
      "4                  0.0       5                 0.0          2   \n",
      "5                  0.0       5                 0.0          2   \n",
      "6                  0.0       5                 0.0          0   \n",
      "7                  0.0       5                 0.0          0   \n",
      "8                  0.0       5                 1.0          5   \n",
      "9                  0.0       5                 0.0          2   \n",
      "10                 1.0       5                 0.0          4   \n",
      "11                 0.0       5                 0.0          3   \n",
      "12                 0.0       5                 1.0          5   \n",
      "13                 0.0       5                 0.0          3   \n",
      "14                 0.0       5                 0.0          0   \n",
      "15                 0.0       5                 0.0          3   \n",
      "16                 0.0       5                 0.0          3   \n",
      "17                 0.0       5                 0.0          0   \n",
      "18                 0.0       5                 0.0          1   \n",
      "\n",
      "    Top Topic Probability  \n",
      "0                    1.00  \n",
      "1                    0.81  \n",
      "2                    1.00  \n",
      "3                    1.00  \n",
      "4                    1.00  \n",
      "5                    1.00  \n",
      "6                    1.00  \n",
      "7                    1.00  \n",
      "8                    1.00  \n",
      "9                    1.00  \n",
      "10                   1.00  \n",
      "11                   1.00  \n",
      "12                   1.00  \n",
      "13                   1.00  \n",
      "14                   1.00  \n",
      "15                   1.00  \n",
      "16                   1.00  \n",
      "17                   1.00  \n",
      "18                   1.00  \n",
      "\n",
      "[19 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe that merges the initial dataframe with the new topics dataframe\r\n",
    "df1 = pd.concat([df,topics_df], axis =1)\r\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\r\n",
    "df1.to_excel(outfile,index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('DevSoftware': virtualenv)",
   "name": "python388jvsc74a57bd0a6c131c26f50ba2ded071a0b6b96b2e36e00e7793e3b3ff9f09232dca713c27c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}